{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871337cd",
   "metadata": {},
   "source": [
    "# Simple Neural Network Music Genre Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db8535",
   "metadata": {},
   "source": [
    "This notebook is dedicated to improving the test accuracy of the network presented in:\n",
    "https://www.kaggle.com/code/aasimahmed04/music-genre-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa677c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\420\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, IPython\n",
    "import librosa.display as lplt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from keras import regularizers\n",
    "\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625cbb3",
   "metadata": {},
   "source": [
    "The labels are base on the following mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff40da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ = {\n",
    "    \"blues\"     : 0,\n",
    "    \"classical\" : 1,\n",
    "    \"country\"   : 2,\n",
    "    \"disco\"     : 3,\n",
    "    \"hiphop\"    : 4,\n",
    "    \"jazz\"      : 5,\n",
    "    \"metal\"     : 6,\n",
    "    \"pop\"       : 7,\n",
    "    \"reggae\"    : 8,\n",
    "    \"rock\"      : 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d248bf",
   "metadata": {},
   "source": [
    "### Loading the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882af414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\420\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\420\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\420\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\420\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load VGGish feature extractor from TensorFlow Hub\n",
    "vggish = hub.load(\"https://tfhub.dev/google/vggish/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240ca65",
   "metadata": {},
   "source": [
    "##### audio samples length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303886e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 30 \n",
    "N = duration - 1 #(N-1 frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d3355",
   "metadata": {},
   "source": [
    "### Functions to load the audio file and generate the feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef38142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract VGGish features from audio files\n",
    "def extract_vggish_features(file_path):\n",
    "    waveform, _ = librosa.load(file_path, sr=16000)  # VGGish expects 16kHz audio\n",
    "    waveform = waveform[:N*16000]  # Ensure the waveform is 1 second long\n",
    "    if len(waveform) < N*16000:\n",
    "        waveform = np.pad(waveform, (0, N*16000 - len(waveform)))\n",
    "    \n",
    "    # VGGish expects a 2D tensor with shape (batch_size, waveform_length)\n",
    "    waveform = waveform[np.newaxis, :]  # Add batch dimension\n",
    "    features = vggish(waveform[0])  # Pass the waveform to VGGish\n",
    "    print(\"features shape = \",features.numpy().shape);\n",
    "    features_ = [];\n",
    "    for i in range(10):\n",
    "        features_.append(features[3*i:3*i+3])\n",
    "    \n",
    "    return np.array(features_).mean(axis=1)\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_dataset(data_dir):\n",
    "    labels = []\n",
    "    features = []\n",
    "    genres = os.listdir(data_dir)\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_dir = os.path.join(data_dir, genre)\n",
    "        genre_dir = genre_dir.replace(\"\\\\\", \"/\")\n",
    "        for file in os.listdir(genre_dir):\n",
    "            file_path = os.path.join(genre_dir, file)\n",
    "            file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "            label = genres.index(genre)\n",
    "            clear_output(wait=True)\n",
    "            print(\"Processing: \",len(features)/10, \"%\")\n",
    "            features.append(extract_vggish_features(file_path))\n",
    "            labels.append(label)\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465536fb",
   "metadata": {},
   "source": [
    "### Path to the audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb2c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data set to you pc and adjust the path accordingly\n",
    "data_dir = r\"C:/Users/420/Desktop/NeuralNetworksFinalProject/genres_original\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b6c8d",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33eed977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  99.8 %\n",
      "features shape =  (30, 128)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47849f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "861ebd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "for label in y:\n",
    "    y_.extend(10*[label])\n",
    "\n",
    "y_ = np.array(y_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f63c7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9990, 128)\n",
      "(9990,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b600b",
   "metadata": {},
   "source": [
    "### Create the .csv file out of the data outside the VGGish network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694d049",
   "metadata": {},
   "source": [
    "This step is in order to not have to process the data through the VGGish model each run\n",
    "(the process is very slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "795b35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to vggish_features_labels_3_sec_extended.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df_features = pd.DataFrame(X)\n",
    "df_labels = pd.DataFrame(y_, columns=['label'])\n",
    "\n",
    "# Combine features and labels into one DataFrame\n",
    "df = pd.concat([df_features, df_labels], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'vggish_features_labels_3_sec_extended.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831c4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
